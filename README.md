# CNN1
CNN1
前言，好久不见，大家有没有想我啊。哈哈。今天我们来随便说说卷积神经网络。
1卷积神经网络的优点
卷积神经网络进行图像分类是深度学习关于图像处理的一个应用，卷积神经网络的优点是能够直接与图像像素进行卷积，从图像像素中提取图像特征，这种处理方式更加接近人类大脑视觉系统的处理方式。另外，卷积神经网络的权值共享属性和pooling层使网络需要训练的参数大大减小，简化了网络模型，提高了训练的效率。

2 卷积神经网络的架构
卷积神经网络与原始神经网络有什么区别呢，现在我分别给他们的架构图。



图 1 普通深度神经网络



图 2 卷积神经网络

哇，大家看，有什么特别的，是不是在输出层与卷积层之间有一个C/S的什么乱七八糟的的东西。其实这个就是卷积神经网络的精髓所在，他的优点不就是可以直接学习像素点，类似于人的眼睛采集信息吗。

输入层:卷积输入层可以直接作用于原始输入数据，对于输入是图像来说，输入数据是图像的像素值。

卷积层:卷积神经网络的卷积层，也叫做特征提取层，包括二个部分。第一部分是真正的卷积层，主要作用是提取输入数据特征。每一个不同的卷积核提取输入数据的特征都不相同，卷积层的卷积核数量越多，就能提取越多输入数据的特征。

第二部分是pooling层，也叫下采样层，主要目的是在保留有用信息的基础上减少数据处理量，加快训练网络的速度。通常情况下，卷积神经网络至少包含二层卷积层(这里把真正的卷积层和下采样层统称为卷积层)，即卷积层，pooling层，卷积层，pooling层。卷积层数越多，在前一层卷积层基础上能够提取更加抽象的特征。

全连接层:可以包含多个全连接层，实际上就是多层感知机的隐含层部分。通常情况下后面层的神经节点都和前一层的每一个神经节点连接，同一层的神经元节点之间是没有连接的。每一层的神经元节点分别通过连接线上的权值进行前向传播，加权组合得到下一层神经元节点的输入。

输出层:输出层神经节点的数目是根据具体应用任务来设定的。如果是分类任务，卷积神经网络输出层通常是一个分类器。

3卷积计算
卷积核类似于人眼对物体进行扫描，所以自然很重要了。其中有离散型卷积核连续性卷积。

连续型卷积：

离散型卷积：

再神经网络中的卷积操作都属于离散卷积，其实际上是一个线性运算，而不是真正意义上的卷积操作，相应的卷积核也可以称为滤波器。卷积核大小确定了图像中参与运算子区域的大小。说白了，卷积核上的参数可以当成权重，就是说，现在每个像素点，对最后的卷积结果的投票能力，权值越大，投票能力也就越大。

3.1 卷积运算和相关运算的区别
例子如下，matlab输入运算：



得到结果：



卷积运算：



得到结果：



从中可以看出，卷积计算其实就是相关计算翻转180°，为什么要这样呢。

相关计算公式：

相关顾名思义就是计算，两个变量之间的相关关系，而卷积则是某个信号对整个系统所产生的影响。

两者相同点是：都是将两个函数映射到一个函数的函数 (f,g)->h。过程也很类似，卷积 是在先将一个函数反转以后进行 滑动相关。

不同点：
1、卷积是对称的 conv(f,g)=conv(g,f)。而滑动相关不对称，ccorr(f,g)~=ccorr(g,f)。
2、卷积是两个系统作用时的响应，（由于对称性，谁作用于谁并不本质）。
滑动相关是衡量两个函数相似度，与相对位置之间的关系。
3、信号处理上卷积可以进行局部操作（就是滤波），例如图像的高斯模糊。系统响应分析。
滑动相关一般用来进行特征检测，比如图像特征提取

3.2 卷积过程


上述图表示，卷积过程，首先输入原始图像，然后用f(x)这个卷积核进行卷积，在加一个偏置，然后用激活函数进行非线性映射，得到初步的卷积结果，现在激活函数一般都用relu，sigmoid用的都比较少了。

3.3下采样过程
既然图像经过卷积之后我们是不是就要用分类器进行分类了呢，显然是不行的，你想一个图像辣么大，网络的训练速度一定会很慢的吧，过拟合肯定也不用说了。那么我们的前辈想到的肯定是降维啊，但是这里我们不叫降维，而是叫下采样，当然降维是我理解的，也不知道准确与否。

下采样就是利用图像的静态性。对相邻的地方进行聚合统计，打个比方就是你额头的地方肯定和你额头的地方很像吧，耳朵也和你耳朵很像吧。

另外，图像的下采样具有不变性。如果下采样区域为特征映射的连续区域，那么得到的下采样单元具有平移不变性，比如图像经过一个小的平移处理后，同样会得到相同的(下采样)特征。在实际应用中，比如物体检测，声音识别等应用中，都希望系统具有平移不变性的特征，因为具有平移不变特征后，即使样本经过平移处理后，标记依然能够被系统识别。

综上所述，下采样的主要作用是:

1、降低图像分辨率;（我理解就是，分辨率越高反而不利于分类，肯定过拟合啊）

2、减少运算数据量;

3、增强网络对图像变化的适应性。

3.4局部连接和权值共享
局部连接与全局连接：

大家看，假设我们爱因斯坦图片是1000x1000像素的图片，那就是一百万的像素点，输入的维度也是一百万，在连接一个相同大小的隐藏层，那么就是一万亿个连接，那还训练个毛线啊，所以我们必须减少权重的数量。

插播一段小广告，来自于百度回答。

上述的回答说白了，就是猫猫的视觉神经中，每一个感受野只接受一小块的区域的信号，对其他东西是不感冒的。那么是不是就启发我们的卷积神经网络了？

局部连接就是根绝上述思想而来的。每一个神经元不需要接收全部像素点的信息，只需要接收局部的像素点进行输入。然后在把所有信息综合起来，那么就可以得到全局信息了。假设感受野大小是10x10，那么是不是就只需要10x10x100=1亿个连接了？

但是呢一亿个好像还是有点大，现在假设我们每个隐藏节点的神经元参数都是一样的，那么参数就只有100个参数，也就是每一个过程就只有10x10那一百个参数。这样整个模型的复杂度就降下来了。（其实我这里一直认为，给每个神经元适当的权重还是不错的，毕竟有的特征（像素）肯定对分类效果影响比较大吧）

这样我们的卷积神经网络特征就构建完毕了。卷积核越多就可以构建更多的高阶特征。

高阶特征类似于这样，这也符合人类看物体的习惯，因为在人的视网膜里物体无非都是边边线线的组合，这些高阶特征经过组合就编程了物体的大致形状。完全不合小样本的概念：比如一个鼠标你只要看过这样的：

那么下次你在见过这样的，你一定知道他是鼠标，无论他多么绚丽，因为在你的脑海里，已经有诸多的高阶特征构成了鼠标，所以分类效果当然very good。

我可没有打广告哦....

4 光说不练假把式，光练不说傻把式。
我们现在来看一下具体的实验效果吧。


以上是测试MNIST手写体的实验，直接输入图片，识别率是88.7%，当然不太高，这是参数没有调好的缘故。

5 总结

今天只是简单介绍了一下卷积神经网络的知识点，等下一章节，我们在介绍目前为止世界上最先进的卷积神经网络改进，大家不见不散哦。
